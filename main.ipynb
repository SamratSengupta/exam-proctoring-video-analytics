{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2';\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import coloredlogs\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "import imutils\n",
    "from imutils import face_utils\n",
    "import deepgaze\n",
    "from deepgaze.head_pose_estimation import CnnHeadPoseEstimator\n",
    "import json\n",
    "from img_json import im2json, json2im\n",
    "\n",
    "from GazeML.src.datasources import Video, Webcam\n",
    "from GazeML.src.models import ELG\n",
    "import GazeML.src.util.gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaze Angle Estimation\n",
    "\n",
    "imgs_list = []\n",
    "if os.path.isdir('img_cap') == False:\n",
    "\tos.mkdir('img_cap')\n",
    "else:\n",
    "\tpath = os.getcwd() + os.sep + 'img_cap'\n",
    "\tfor f in os.listdir(path):\n",
    "\t\tos.remove(path + os.sep + f)\n",
    "\n",
    "if os.path.isdir('outputs') == False:\n",
    "\tos.mkdir('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coloredlogs.install(\n",
    "\tdatefmt='%d/%m %H:%M',\n",
    "\tfmt='%(asctime)s %(levelname)s %(message)s',\n",
    "\tlevel='INFO',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "from tensorflow.python.client import device_lib\n",
    "gpu_available = False\n",
    "try:\n",
    "\tgpus = [d for d in device_lib.list_local_devices()\n",
    "\t\t\tif d.device_type == 'GPU']\n",
    "\tgpu_available = len(gpus) > 0\n",
    "except:\n",
    "\tpass\n",
    "\n",
    "print(gpu_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "# Declare some parameters\n",
    "batch_size = 2\n",
    "\n",
    "# Define webcam stream data source\n",
    "# Change data_format='NHWC' if not using CUDA\n",
    "data_source = Webcam(tensorflow_session=session, batch_size=batch_size,\n",
    "\t\t\t\t\t camera_id=0, fps=60,\n",
    "\t\t\t\t\t data_format='NCHW' if gpu_available else 'NHWC',\n",
    "\t\t\t\t\t eye_image_shape=(36, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ELG(\n",
    "\tsession, train_data={'videostream': data_source},\n",
    "\tfirst_layer_stride=1,\n",
    "\tnum_modules=2,\n",
    "\tnum_feature_maps=32,\n",
    "\tlearning_schedule=[\n",
    "\t\t{\n",
    "\t\t\t'loss_terms_to_optimize': {'dummy': ['hourglass', 'radius']},\n",
    "\t\t},\n",
    "\t],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin visualization thread\n",
    "inferred_stuff_queue = queue.Queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visualize_output():\n",
    "\tlast_frame_index = 0\n",
    "\tlast_frame_time = time.time()\n",
    "\tfps_history = []\n",
    "\tall_gaze_histories = [list() for _ in range(2)]\n",
    "\tgaze_history_max_len = 10\n",
    "\tprev = time.time() + 1\n",
    "\ti = 0\n",
    "\t\n",
    "\tdef make_dict(img):\n",
    "\t\t# Store gaze data in json file\n",
    "\t\tnonlocal prev, i\n",
    "\n",
    "\t\tif time.time() - prev > 1:\n",
    "\t\t\tghl = all_gaze_histories[0]\n",
    "\t\t\tif len(ghl) > gaze_history_max_len:\n",
    "\t\t\t\tghl = ghl[-gaze_history_max_len:]\n",
    "\t\t\tleft = np.asarray(ghl)\n",
    "\n",
    "\t\t\tghr = all_gaze_histories[1]\n",
    "\t\t\tif len(ghr) > gaze_history_max_len:\n",
    "\t\t\t\tghr = ghr[-gaze_history_max_len:]\n",
    "\t\t\tright = np.asarray(ghr)\n",
    "\t\t\tsys.stdout.write(f\"\\rCaptured frame {i}\")\n",
    "\t\t\tsys.stdout.flush()\n",
    "\t\t\ti += 1\n",
    "\t\t\tprev = time.time()\n",
    "\t\t\tcv2.imwrite(os.getcwd() + os.sep + 'img_cap' + os.sep + f'{i}.jpg', img)\n",
    "\n",
    "\t\t\tjstr = im2json(img)\n",
    "\t\t\timg_dict = json.loads(jstr)\n",
    "\t\t\timg_dict['index'] = i\n",
    "\t\t\timg_dict['left_eye'] = {\n",
    "\t\t\t\t'pitch' : np.mean(left, axis=0)[0],\n",
    "\t\t\t\t'yaw' : np.mean(left, axis=0)[1]\n",
    "\t\t\t} if left.any() else None\n",
    "\t\t\timg_dict['right_eye'] = {\n",
    "\t\t\t\t'pitch' : np.mean(right, axis=0)[0],\n",
    "\t\t\t\t'yaw' : np.mean(right, axis=0)[1]\n",
    "\t\t\t} if right.any() else None\n",
    "\n",
    "\t\t\timgs_list.append(img_dict)\n",
    "\t\t\t\n",
    "\tprint(\"\\nGaze angle estimation started\")\n",
    "\n",
    "\twhile True:\n",
    "\t\t# If no output to visualize, show unannotated frame\n",
    "\t\tif inferred_stuff_queue.empty():\n",
    "\t\t\tnext_frame_index = last_frame_index + 1\n",
    "\t\t\tif next_frame_index in data_source._frames:\n",
    "\t\t\t\tnext_frame = data_source._frames[next_frame_index]\n",
    "\t\t\t\tif 'faces' in next_frame and len(next_frame['faces']) == 0:\n",
    "\t\t\t\t\tcv2.imshow('vis', next_frame['bgr'])\n",
    "\t\t\t\t\tlast_frame_index = next_frame_index\n",
    "\t\t\t\tmake_dict(next_frame['bgr'])\n",
    "\t\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\t\tprev = time.time() - 1\n",
    "\t\t\t\tmake_dict(next_frame['bgr']*0)\n",
    "\t\t\t\tprint()\n",
    "\t\t\t\tcv2.destroyAllWindows()\n",
    "\t\t\t\treturn\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Get output from neural network and visualize\n",
    "\t\toutput = inferred_stuff_queue.get()\n",
    "\t\tbgr = None\n",
    "\t\tfor j in range(batch_size):\n",
    "\t\t\tframe_index = output['frame_index'][j]\n",
    "\t\t\tif frame_index not in data_source._frames:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tframe = data_source._frames[frame_index]\n",
    "\t\t\tif j == 0 and output['eye_index'][j] == 0:\n",
    "\t\t\t\timg = frame['bgr'].copy()\n",
    "\n",
    "\t\t\t# Decide which landmarks are usable\n",
    "\t\t\theatmaps_amax = np.amax(output['heatmaps'][j, :].reshape(-1, 18), axis=0)\n",
    "\t\t\tcan_use_eye = np.all(heatmaps_amax > 0.7)\n",
    "\t\t\tcan_use_eyelid = np.all(heatmaps_amax[0:8] > 0.75)\n",
    "\t\t\tcan_use_iris = np.all(heatmaps_amax[8:16] > 0.8)\n",
    "\n",
    "\t\t\tstart_time = time.time()\n",
    "\t\t\teye_index = output['eye_index'][j]\n",
    "\t\t\tbgr = frame['bgr']\n",
    "\t\t\teye = frame['eyes'][eye_index]\n",
    "\t\t\teye_image = eye['image']\n",
    "\t\t\teye_side = eye['side']\n",
    "\t\t\teye_landmarks = output['landmarks'][j, :]\n",
    "\t\t\teye_radius = output['radius'][j][0]\n",
    "\t\t\tif eye_side == 'left':\n",
    "\t\t\t\teye_landmarks[:, 0] = eye_image.shape[1] - eye_landmarks[:, 0]\n",
    "\t\t\t\teye_image = np.fliplr(eye_image)\n",
    "\n",
    "\t\t\t# Embed eye image and annotate for picture-in-picture\n",
    "\t\t\teye_upscale = 2\n",
    "\t\t\teye_image_raw = cv2.cvtColor(cv2.equalizeHist(eye_image), cv2.COLOR_GRAY2BGR)\n",
    "\t\t\teye_image_raw = cv2.resize(eye_image_raw, (0, 0), fx=eye_upscale, fy=eye_upscale)\n",
    "\t\t\teye_image_annotated = np.copy(eye_image_raw)\n",
    "\t\t\tif can_use_eyelid:\n",
    "\t\t\t\tcv2.polylines(\n",
    "\t\t\t\t\teye_image_annotated,\n",
    "\t\t\t\t\t[np.round(eye_upscale*eye_landmarks[0:8]).astype(np.int32)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t .reshape(-1, 1, 2)],\n",
    "\t\t\t\t\tisClosed=True, color=(255, 255, 0), thickness=1, lineType=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\t\t\tif can_use_iris:\n",
    "\t\t\t\tcv2.polylines(\n",
    "\t\t\t\t\teye_image_annotated,\n",
    "\t\t\t\t\t[np.round(eye_upscale*eye_landmarks[8:16]).astype(np.int32)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  .reshape(-1, 1, 2)],\n",
    "\t\t\t\t\tisClosed=True, color=(0, 255, 255), thickness=1, lineType=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tcv2.drawMarker(\n",
    "\t\t\t\t\teye_image_annotated,\n",
    "\t\t\t\t\ttuple(np.round(eye_upscale*eye_landmarks[16, :]).astype(np.int32)),\n",
    "\t\t\t\t\tcolor=(0, 255, 255), markerType=cv2.MARKER_CROSS, markerSize=4,\n",
    "\t\t\t\t\tthickness=1, line_type=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\t\t\tface_index = int(eye_index / 2)\n",
    "\t\t\teh, ew, _ = eye_image_raw.shape\n",
    "\t\t\tv0 = face_index * 2 * eh\n",
    "\t\t\tv1 = v0 + eh\n",
    "\t\t\tv2 = v1 + eh\n",
    "\t\t\tu0 = 0 if eye_side == 'left' else ew\n",
    "\t\t\tu1 = u0 + ew\n",
    "\t\t\tbgr[v0:v1, u0:u1] = eye_image_raw\n",
    "\t\t\tbgr[v1:v2, u0:u1] = eye_image_annotated\n",
    "\n",
    "\t\t\t# Visualize preprocessing results\n",
    "\t\t\tframe_landmarks = (frame['smoothed_landmarks']\n",
    "\t\t\t\t\t\t\t   if 'smoothed_landmarks' in frame\n",
    "\t\t\t\t\t\t\t   else frame['landmarks'])\n",
    "\t\t\tfor f, face in enumerate(frame['faces']):\n",
    "\t\t\t\tfor landmark in frame_landmarks[f][:-1]:\n",
    "\t\t\t\t\tcv2.drawMarker(bgr, tuple(np.round(landmark).astype(np.int32)),\n",
    "\t\t\t\t\t\t\t\t  color=(0, 0, 255), markerType=cv2.MARKER_STAR,\n",
    "\t\t\t\t\t\t\t\t  markerSize=2, thickness=1, line_type=cv2.LINE_AA)\n",
    "\t\t\t\tcv2.rectangle(\n",
    "\t\t\t\t\tbgr, tuple(np.round(face[:2]).astype(np.int32)),\n",
    "\t\t\t\t\ttuple(np.round(np.add(face[:2], face[2:])).astype(np.int32)),\n",
    "\t\t\t\t\tcolor=(0, 255, 255), thickness=1, lineType=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t# Transform predictions\n",
    "\t\t\teye_landmarks = np.concatenate([eye_landmarks,\n",
    "\t\t\t\t\t\t\t\t\t\t\t[[eye_landmarks[-1, 0] + eye_radius,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  eye_landmarks[-1, 1]]]])\n",
    "\t\t\teye_landmarks = np.asmatrix(np.pad(eye_landmarks, ((0, 0), (0, 1)),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   'constant', constant_values=1.0))\n",
    "\t\t\teye_landmarks = (eye_landmarks *\n",
    "\t\t\t\t\t\t\t eye['inv_landmarks_transform_mat'].T)[:, :2]\n",
    "\t\t\teye_landmarks = np.asarray(eye_landmarks)\n",
    "\t\t\teyelid_landmarks = eye_landmarks[0:8, :]\n",
    "\t\t\tiris_landmarks = eye_landmarks[8:16, :]\n",
    "\t\t\tiris_centre = eye_landmarks[16, :]\n",
    "\t\t\teyeball_centre = eye_landmarks[17, :]\n",
    "\t\t\teyeball_radius = np.linalg.norm(eye_landmarks[18, :] -\n",
    "\t\t\t\t\t\t\t\t\t\t\teye_landmarks[17, :])\n",
    "\n",
    "\t\t\t# Smooth and visualize gaze direction\n",
    "\t\t\tnum_total_eyes_in_frame = len(frame['eyes'])\n",
    "\t\t\tif len(all_gaze_histories) < num_total_eyes_in_frame:\n",
    "\t\t\t\tall_gaze_histories = [list() for _ in range(num_total_eyes_in_frame)]\n",
    "\t\t\tgaze_history = all_gaze_histories[eye_index]\n",
    "\t\t\tif can_use_eye:\n",
    "\t\t\t\t# Visualize landmarks\n",
    "\t\t\t\tcv2.drawMarker(  # Eyeball centre\n",
    "\t\t\t\t\tbgr, tuple(np.round(eyeball_centre).astype(np.int32)),\n",
    "\t\t\t\t\tcolor=(0, 255, 0), markerType=cv2.MARKER_CROSS, markerSize=4,\n",
    "\t\t\t\t\tthickness=1, line_type=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\t\t\t\ti_x0, i_y0 = iris_centre\n",
    "\t\t\t\te_x0, e_y0 = eyeball_centre\n",
    "\t\t\t\ttheta = -np.arcsin(np.clip((i_y0 - e_y0) / eyeball_radius, -1.0, 1.0))\n",
    "\t\t\t\tphi = np.arcsin(np.clip((i_x0 - e_x0) / (eyeball_radius * -np.cos(theta)),\n",
    "\t\t\t\t\t\t\t\t\t\t-1.0, 1.0))\n",
    "\t\t\t\tcurrent_gaze = np.array([theta, phi])\n",
    "\t\t\t\tgaze_history.append(current_gaze)\n",
    "\t\t\t\tif len(gaze_history) > gaze_history_max_len:\n",
    "\t\t\t\t\tgaze_history = gaze_history[-gaze_history_max_len:]\n",
    "\t\t\t\tGazeML.src.util.gaze.draw_gaze(bgr, iris_centre, np.mean(gaze_history, axis=0),\n",
    "\t\t\t\t\t\t\t\t\tlength=120.0, thickness=1)\n",
    "\n",
    "\t\t\t# else:\n",
    "\t\t\t# \tgaze_history.clear()\n",
    "\n",
    "\t\t\tif can_use_eyelid:\n",
    "\t\t\t\tcv2.polylines(\n",
    "\t\t\t\t\tbgr, [np.round(eyelid_landmarks).astype(np.int32).reshape(-1, 1, 2)],\n",
    "\t\t\t\t\tisClosed=True, color=(255, 255, 0), thickness=1, lineType=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tif can_use_iris:\n",
    "\t\t\t\tcv2.polylines(\n",
    "\t\t\t\t\tbgr, [np.round(iris_landmarks).astype(np.int32).reshape(-1, 1, 2)],\n",
    "\t\t\t\t\tisClosed=True, color=(0, 255, 255), thickness=1, lineType=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\t\t\t\tcv2.drawMarker(\n",
    "\t\t\t\t\tbgr, tuple(np.round(iris_centre).astype(np.int32)),\n",
    "\t\t\t\t\tcolor=(0, 255, 255), markerType=cv2.MARKER_CROSS, markerSize=4,\n",
    "\t\t\t\t\tthickness=1, line_type=cv2.LINE_AA,\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\tdtime = 1e3*(time.time() - start_time)\n",
    "\t\t\tif 'visualization' not in frame['time']:\n",
    "\t\t\t\tframe['time']['visualization'] = dtime\n",
    "\t\t\telse:\n",
    "\t\t\t\tframe['time']['visualization'] += dtime\n",
    "\n",
    "\t\t\tdef _dtime(before_id, after_id):\n",
    "\t\t\t\treturn int(1e3 * (frame['time'][after_id] - frame['time'][before_id]))\n",
    "\n",
    "\t\t\tdef _dstr(title, before_id, after_id):\n",
    "\t\t\t\treturn '%s: %dms' % (title, _dtime(before_id, after_id))\n",
    "\n",
    "\t\t\tif eye_index == len(frame['eyes']) - 1:\n",
    "\t\t\t\t# Calculate timings\n",
    "\t\t\t\tframe['time']['after_visualization'] = time.time()\n",
    "\t\t\t\tfps = int(np.round(1.0 / (time.time() - last_frame_time)))\n",
    "\t\t\t\tfps_history.append(fps)\n",
    "\t\t\t\tif len(fps_history) > 60:\n",
    "\t\t\t\t\tfps_history = fps_history[-60:]\n",
    "\t\t\t\tfps_str = '%d FPS' % np.mean(fps_history)\n",
    "\t\t\t\tlast_frame_time = time.time()\n",
    "\t\t\t\tfh, fw, _ = bgr.shape\n",
    "\t\t\t\tcv2.putText(bgr, fps_str, org=(fw - 110, fh - 20),\n",
    "\t\t\t\t\t\t   fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=0.8,\n",
    "\t\t\t\t\t\t   color=(0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n",
    "\t\t\t\tcv2.putText(bgr, fps_str, org=(fw - 111, fh - 21),\n",
    "\t\t\t\t\t\t   fontFace=cv2.FONT_HERSHEY_DUPLEX, fontScale=0.79,\n",
    "\t\t\t\t\t\t   color=(255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\t\t\t\tcv2.imshow('vis', bgr)\n",
    "\t\t\t\tlast_frame_index = frame_index\n",
    "\n",
    "\t\t\t\t# Quit?\n",
    "\t\t\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\t\t\tprev = time.time() - 1\n",
    "\t\t\t\t\tmake_dict(img)\n",
    "\t\t\t\t\tcv2.destroyAllWindows()\n",
    "\t\t\t\t\treturn\n",
    "\n",
    "\t\tmake_dict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_thread = threading.Thread(target=_visualize_output, name='visualization')\n",
    "visualize_thread.daemon = True\n",
    "visualize_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do inference forever\n",
    "infer = model.inference_generator()\n",
    "while True:\n",
    "\toutput = next(infer)\n",
    "\tfor frame_index in np.unique(output['frame_index']):\n",
    "\t\tif frame_index not in data_source._frames:\n",
    "\t\t\tcontinue\n",
    "\t\tframe = data_source._frames[frame_index]\n",
    "\t\tif 'inference' in frame['time']:\n",
    "\t\t\tframe['time']['inference'] += output['inference_time']\n",
    "\t\telse:\n",
    "\t\t\tframe['time']['inference'] = output['inference_time']\n",
    "\tinferred_stuff_queue.put_nowait(output)\n",
    "\n",
    "\tif not visualize_thread.isAlive():\n",
    "\t\tbreak\n",
    "\n",
    "\tif not data_source._open:\n",
    "\t\tbreak\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + os.sep + 'outputs' + os.sep + 'img_stats.json'\n",
    "with open(path, \"w\") as p: \n",
    "\tjson.dump(imgs_list, p, indent = 4)\n",
    "\n",
    "print('Gaze angle estimation complete\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head Pose Estimation\n",
    "\n",
    "models = os.getcwd() + os.sep + 'models'\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(models + os.sep + 'shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "head_pose_estimator = CnnHeadPoseEstimator(sess)\n",
    "head_pose_estimator.load_pitch_variables(models + os.sep + 'pitch.tf')\n",
    "head_pose_estimator.load_yaw_variables(models + os.sep + 'yaw.tf')\n",
    "head_pose_estimator.load_roll_variables(models + os.sep + 'roll.tf')\n",
    "\n",
    "path = os.getcwd() + os.sep + 'outputs' + os.sep + 'img_stats.json'\n",
    "with open(path, \"r\") as p: \n",
    "\tdata_list = json.load(p)\n",
    "\n",
    "print(\"Head pose estimation started\")\n",
    "\n",
    "for data in data_list:\n",
    "\tframe = json2im(json.dumps(data))\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\t(fh, fw) = frame.shape[:2]\n",
    "\n",
    "\tfaces = detector(gray, 0)\n",
    "\n",
    "\tfor face in faces:\n",
    "\t\t(x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "\t\tcv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\t\timage = frame[y:y + h, x:x + w]\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\timage = cv2.resize(image, (480,480))\n",
    "\t\texcept:\n",
    "\t\t\tprint('Exception')\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tpitch = head_pose_estimator.return_pitch(image,radians=True)[0][0][0]\n",
    "\t\tyaw = head_pose_estimator.return_yaw(image,radians=True)[0][0][0]\n",
    "\t\troll = head_pose_estimator.return_roll(image,radians=True)[0][0][0]\n",
    "\n",
    "\t\tsys.stdout.write(f\"\\rProcessed frame {data['index']}\")\n",
    "\t\tsys.stdout.flush()\n",
    "\n",
    "\t\tFONT = cv2.FONT_HERSHEY_DUPLEX\n",
    "\n",
    "\t\tdata['pose'] = {\n",
    "\t\t\t'pitch' : float(pitch),\n",
    "\t\t\t'yaw' : float(yaw),\n",
    "\t\t\t'roll' : float(roll)\n",
    "\t\t}\n",
    "\n",
    "\tif not faces:\n",
    "\t\tdata['pose'] = None\n",
    "\n",
    "print(\"\\nHead pose estimation complete\\n\")\n",
    "\n",
    "path = os.getcwd() + os.sep + 'outputs' + os.sep + 'img_stats.json'\n",
    "with open(path, \"w\") as p: \n",
    "\tjson.dump(data_list, p, indent = 4)\n",
    "\t\t\t\t\t\t \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saved frame statistics to outputs/img_stats.json')\n",
    "print('Saved images to img_cap/')\n",
    "print('Done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
